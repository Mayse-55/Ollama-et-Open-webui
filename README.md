# üß† Installation LLM : OLLAMA + Open-WebUI
[![Ollama](https://img.shields.io/badge/Ollama-LLM%20Runner-black?style=flat-square&logo=terminal&logoColor=white)](https://ollama.com/)

## üì¶ Pr√©requis

* Syst√®me √† jour : **Debian** ou **Ubuntu**
* Carte graphique **NVIDIA** avec pilotes propri√©taires install√©s
* Acc√®s administrateur (**sudo**)
* Acc√®s SSH au serveur via **OpenSSH** *(optionnel)*

> [!caution]
> ‚úÖ Cette documentation a √©t√© **test√©e et valid√©e** dans un environnement conforme aux pr√©requis.  
> ‚ùå‚Äã‚Äã‚Äã Si vous rencontrez des **probl√®mes**, il est probable que cela provienne **de votre configuration**.
---

## ‚öôÔ∏è Installation de Ollama

### 1Ô∏è‚É£ Mettre √† jour le syst√®me

```bash
sudo apt update && sudo apt dist-upgrade -y
```

### 2Ô∏è‚É£ Installer les outils de base

```bash
sudo apt install -y curl gpg && sudo apt update
```

### 3Ô∏è‚É£ Ajouter le d√©p√¥t NVIDIA Container Toolkit

```bash
sudo curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | \
  sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg && \
  curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
```

### 4Ô∏è‚É£ D√©commenter la ligne "experimental" (si pr√©sente)

```bash
sudo sed -i -e '/experimental/ s/^#//g' /etc/apt/sources.list.d/nvidia-container-toolkit.list
```

### 5Ô∏è‚É£ Mettre √† jour les d√©p√¥ts

```bash
sudo apt update
```

### ‚ö†Ô∏è Supprimer les d√©p√¥ts en double (si message d‚Äôerreur)

```bash
sudo rm /etc/apt/sources.list.d/contrib.list
sudo apt update
```

### 6Ô∏è‚É£ Installer NVIDIA Container Toolkit

```bash
sudo apt install -y nvidia-container-toolkit
```

### 7Ô∏è‚É£ Installer Ollama

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 8Ô∏è‚É£ Red√©marrer le serveur

```bash
sudo reboot
```

‚ùå‚Äã‚Äã Si vous voyez `WARNING: No NVIDIA/AMD GPU detected`, la carte graphique n‚Äôest pas d√©tect√©e.

### 9Ô∏è‚É£ Installer le moniteur GPU (nvtop)

```bash
sudo apt install nvtop && sudo nvtop
```

### üîü Tester Ollama avec Mistral

```bash
ollama run mistral
```

---

## üåê Installation de Open-WebUI

### 1Ô∏è‚É£ Mise √† jour des paquets

```bash
sudo apt update
```

### 2Ô∏è‚É£ Installer Docker et Docker Compose

```bash
sudo apt install docker docker-compose
```

### 3Ô∏è‚É£ Lancer Open-WebUI via Docker

```bash
sudo docker run -d -p 3000:8080 \
  --gpus all \
  --add-host=host.docker.internal:host-gateway \
  -v open-webui:/app/backend/data \
  --name open-webui \
  --restart always \
  ghcr.io/open-webui/open-webui:cuda
```

üêå Cela peut prendre un certain temps

### 4Ô∏è‚É£ Configurer systemd pour Ollama

```bash
sudo nano /etc/systemd/system/ollama.service
```

Ajouter :

```ini
[Service]
Environment="PATH=$PATH"
Environment="OLLAMA_HOST=0.0.0.0"
```

### 5Ô∏è‚É£ Lancer Ollama avec systemd

```bash
sudo systemctl daemon-reexec
sudo systemctl enable ollama.service
sudo systemctl restart ollama.service
```

### 6Ô∏è‚É£ Acc√©der √† l'interface WebUI

Depuis un navigateur :

```
http://<IP-DU-SERVEUR>:3000
```

Ex√©cuter √©galement le moniteur GPU dans la console du serveur :

```bash
sudo nvtop
```

### 7Ô∏è‚É£ Ajouter des mod√®les dans Open-WebUI

![demo](https://github.com/user-attachments/assets/14220a3a-0ee0-4892-a604-4e4b4d97c347)

---

## üîÑ Mise √† jour d‚ÄôOpen-WebUI

### 1Ô∏è‚É£ Stopper et supprimer l'ancien conteneur

```bash
sudo docker stop open-webui
sudo docker rm open-webui
```

### 2Ô∏è‚É£ Mettre √† jour l'image

```bash
sudo docker pull ghcr.io/open-webui/open-webui:cuda
```

### 3Ô∏è‚É£ Relancer le conteneur

```bash
sudo docker run -d -p 3000:8080 \
  --gpus all \
  --add-host=host.docker.internal:host-gateway \
  -v open-webui:/app/backend/data \
  --name open-webui \
  --restart always \
  ghcr.io/open-webui/open-webui:cuda
```
